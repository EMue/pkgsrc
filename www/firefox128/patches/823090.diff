# HG changeset patch
# User Cameron Kaiser <spectre@floodgap.com>
# Date 1723177514 25200
#      Thu Aug 08 21:25:14 2024 -0700
# Node ID 9a197bcbc77065b82aa15f320683bab1a6ec9d8b
# Parent  553c3f1c48b455ac22aa3f907676616e99e41c67
jit ppc64 patches

diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit-test/tests/gc/gcparam.js
--- a/js/src/jit-test/tests/gc/gcparam.js	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit-test/tests/gc/gcparam.js	Thu Aug 08 21:25:14 2024 -0700
@@ -25,17 +25,19 @@
 testGetParam("totalChunks");
 testGetParam("nurseryBytes");
 testGetParam("majorGCNumber");
 testGetParam("minorGCNumber");
 testGetParam("chunkBytes");
 testGetParam("helperThreadCount");
 
 testChangeParam("maxBytes");
-testChangeParam("minNurseryBytes", 16 * 1024);
+// This cannot be lower than 64K due to 64K page systems, like some ppc64le
+// machines in Linux.
+testChangeParam("minNurseryBytes", 64 * 1024);
 testChangeParam("maxNurseryBytes", 1024 * 1024);
 testChangeParam("incrementalGCEnabled");
 testChangeParam("perZoneGCEnabled");
 testChangeParam("sliceTimeBudgetMS");
 testChangeParam("highFrequencyTimeLimit");
 testChangeParam("smallHeapSizeMax");
 testChangeParam("largeHeapSizeMin");
 testChangeParam("highFrequencySmallHeapGrowth");
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit-test/tests/gc/oomInRegExp2.js
--- a/js/src/jit-test/tests/gc/oomInRegExp2.js	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit-test/tests/gc/oomInRegExp2.js	Thu Aug 08 21:25:14 2024 -0700
@@ -1,3 +1,6 @@
+// |jit-test| skip-if: getBuildConfiguration().ppc64
+// On ppc64, this will never exhaust memory before timing out.
+
 oomTest(() => assertEq("foobar\xff5baz\u1200".search(/bar\u0178\d/i), 3), {keepFailing: true});
 oomTest(() => assertEq((/(?!(?!(?!6)[\Wc]))/i).test(), false), {keepFailing: true});
 oomTest(() => assertEq((/bar\u0178\d/i).exec("foobar\xff5baz\u1200") != null, true), {keepFailing: true});
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit-test/tests/modules/bug1670236.js
--- a/js/src/jit-test/tests/modules/bug1670236.js	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit-test/tests/modules/bug1670236.js	Thu Aug 08 21:25:14 2024 -0700
@@ -1,5 +1,8 @@
+// |jit-test| skip-if: getBuildConfiguration().ppc64
+// On ppc64, this will never exhaust memory before timing out.
+
 o0=r=/x/;
 this.toString=(function() {
     evaluate("",({ element:o0 }));
 })
 oomTest(String.prototype.charCodeAt,{ keepFailing:true })
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit-test/tests/promise/unhandled-rejections-oom.js
--- a/js/src/jit-test/tests/promise/unhandled-rejections-oom.js	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit-test/tests/promise/unhandled-rejections-oom.js	Thu Aug 08 21:25:14 2024 -0700
@@ -1,3 +1,4 @@
-// |jit-test| allow-oom
+// |jit-test| allow-oom; skip-if: getBuildConfiguration().ppc64
+// On ppc64, this will never exhaust memory before timing out.
 
 oomTest(async function() {}, { keepFailing: true });
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/Assembler.h
--- a/js/src/jit/Assembler.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/Assembler.h	Thu Aug 08 21:25:14 2024 -0700
@@ -18,16 +18,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/Assembler-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/Assembler-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/Assembler-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/Assembler-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/Assembler-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/Assembler-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/Assembler-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/BaselineIC.cpp
--- a/js/src/jit/BaselineIC.cpp	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/BaselineIC.cpp	Thu Aug 08 21:25:14 2024 -0700
@@ -107,17 +107,18 @@
 };
 
 AllocatableGeneralRegisterSet BaselineICAvailableGeneralRegs(size_t numInputs) {
   AllocatableGeneralRegisterSet regs(GeneralRegisterSet::All());
   MOZ_ASSERT(!regs.has(FramePointer));
 #if defined(JS_CODEGEN_ARM)
   MOZ_ASSERT(!regs.has(ICTailCallReg));
   regs.take(BaselineSecondScratchReg);
-#elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
+#elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) || \
+      defined(JS_CODEGEN_PPC64)
   MOZ_ASSERT(!regs.has(ICTailCallReg));
   MOZ_ASSERT(!regs.has(BaselineSecondScratchReg));
 #elif defined(JS_CODEGEN_ARM64)
   MOZ_ASSERT(!regs.has(PseudoStackPointer));
   MOZ_ASSERT(!regs.has(RealStackPointer));
   MOZ_ASSERT(!regs.has(ICTailCallReg));
 #endif
   regs.take(ICStubReg);
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/CodeGenerator.cpp
--- a/js/src/jit/CodeGenerator.cpp	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/CodeGenerator.cpp	Thu Aug 08 21:25:14 2024 -0700
@@ -2124,16 +2124,23 @@
   masm.storePtr(temp2, matchesAddress);
   masm.storePtr(lastIndex, startIndexAddress);
 
   // Execute the RegExp.
   masm.computeEffectiveAddress(
       Address(FramePointer, inputOutputDataStartOffset), temp2);
   masm.PushRegsInMask(volatileRegs);
   masm.setupUnalignedABICall(temp3);
+#if defined(JS_CODEGEN_PPC64)
+  // temp1 aliases argregs on this platform, so we need to reuse temp3 again
+  // or we'll stomp on the code pointer when we pass the first ABI argument.
+  // Everything gets clobbered anyway!
+  masm.xs_mr(temp3, codePointer);
+  codePointer = temp3;
+#endif
   masm.passABIArg(temp2);
   masm.callWithABI(codePointer);
   masm.storeCallInt32Result(temp1);
   masm.PopRegsInMask(volatileRegs);
 
   masm.bind(&checkSuccess);
   masm.branch32(Assembler::Equal, temp1,
                 Imm32(int32_t(RegExpRunStatus::Success_NotFound)), notFound);
@@ -14851,17 +14858,18 @@
 
   // We're out-of-bounds. We only handle the index == initlength case.
   // If index > initializedLength, bail out. Note that this relies on the
   // condition flags sticking from the incoming branch.
   // Also note: this branch does not need Spectre mitigations, doing that for
   // the capacity check below is sufficient.
   Label allocElement, addNewElement;
 #if defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) || \
-    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64) || \
+    defined(JS_CODEGEN_PPC64)
   // Had to reimplement for MIPS because there are no flags.
   bailoutCmp32(Assembler::NotEqual, initLength, index, ins->snapshot());
 #else
   bailoutIf(Assembler::NotEqual, ins->snapshot());
 #endif
 
   // If index < capacity, we can add a dense element inline. If not, we need
   // to allocate more elements first.
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/CodeGenerator.h
--- a/js/src/jit/CodeGenerator.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/CodeGenerator.h	Thu Aug 08 21:25:14 2024 -0700
@@ -22,16 +22,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/CodeGenerator-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/CodeGenerator-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/CodeGenerator-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/CodeGenerator-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/CodeGenerator-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/CodeGenerator-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/CodeGenerator-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/FlushICache.h
--- a/js/src/jit/FlushICache.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/FlushICache.h	Thu Aug 08 21:25:14 2024 -0700
@@ -19,17 +19,18 @@
 #if defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_X64)
 
 inline void FlushICache(void* code, size_t size) {
   // No-op. Code and data caches are coherent on x86 and x64.
 }
 
 #elif (defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_ARM64)) ||   \
     (defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)) || \
-    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64) || \
+    defined(JS_CODEGEN_PPC64)
 
 // Invalidate the given code range from the icache. This will also flush the
 // execution context for this core. If this code is to be executed on another
 // thread, that thread must perform an execution context flush first using
 // `FlushExecutionContext` below.
 extern void FlushICache(void* code, size_t size);
 
 #elif defined(JS_CODEGEN_NONE) || defined(JS_CODEGEN_WASM32)
@@ -37,17 +38,18 @@
 inline void FlushICache(void* code, size_t size) { MOZ_CRASH(); }
 
 #else
 #  error "Unknown architecture!"
 #endif
 
 #if (defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_X64)) ||       \
     (defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)) || \
-    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64) || \
+    defined(JS_CODEGEN_PPC64)
 
 inline void FlushExecutionContext() {
   // No-op. Execution context is coherent with instruction cache.
 }
 inline bool CanFlushExecutionContextForAllThreads() { return true; }
 inline void FlushExecutionContextForAllThreads() {
   // No-op. Execution context is coherent with instruction cache.
 }
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/JitFrames.cpp
--- a/js/src/jit/JitFrames.cpp	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/JitFrames.cpp	Thu Aug 08 21:25:14 2024 -0700
@@ -1765,17 +1765,25 @@
 
     case RValueAllocation::CST_NULL:
       return NullValue();
 
     case RValueAllocation::DOUBLE_REG:
       return DoubleValue(fromRegister<double>(alloc.fpuReg()));
 
     case RValueAllocation::ANY_FLOAT_REG:
+#if defined(JS_CODEGEN_PPC64)
+      // There is no (simple) way from the ISA to determine if an arbitrary
+      // FPR contains a float or a double since the ISA treats them largely
+      // synonymously, so the MachineState will always contain a double even
+      // if it's encoding a float.
+      return Float32Value((float)fromRegister<double>(alloc.fpuReg()));
+#else
       return Float32Value(fromRegister<float>(alloc.fpuReg()));
+#endif
 
     case RValueAllocation::ANY_FLOAT_STACK:
       return Float32Value(ReadFrameFloat32Slot(fp_, alloc.stackOffset()));
 
     case RValueAllocation::TYPED_REG:
       return FromTypedPayload(alloc.knownType(), fromRegister(alloc.reg2()));
 
     case RValueAllocation::TYPED_STACK: {
@@ -2375,20 +2383,21 @@
     uintptr_t* addr = state_.as<SafepointState>().addressOfRegister(reg);
     return *addr;
   }
   MOZ_CRASH("Invalid state");
 }
 
 template <typename T>
 T MachineState::read(FloatRegister reg) const {
-#if !defined(JS_CODEGEN_RISCV64)
+#if !defined(JS_CODEGEN_RISCV64) && !defined(JS_CODEGEN_PPC64)
   MOZ_ASSERT(reg.size() == sizeof(T));
 #else
   // RISCV64 always store FloatRegister as 64bit.
+  // So does Power ISA (see SnapshotIterator::allocationValue).
   MOZ_ASSERT(reg.size() == sizeof(double));
 #endif
 
 #if !defined(JS_CODEGEN_NONE) && !defined(JS_CODEGEN_WASM32)
   if (state_.is<BailoutState>()) {
     uint32_t offset = reg.getRegisterDumpOffsetInBytes();
     MOZ_ASSERT((offset % sizeof(T)) == 0);
     MOZ_ASSERT((offset + sizeof(T)) <= sizeof(RegisterDump::FPUArray));
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/JitFrames.h
--- a/js/src/jit/JitFrames.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/JitFrames.h	Thu Aug 08 21:25:14 2024 -0700
@@ -119,16 +119,26 @@
 
   // The exception was caught by a wasm catch handler.
   // Restore state and jump to it.
   WasmCatch
 };
 
 // Data needed to recover from an exception.
 struct ResumeFromException {
+#if defined(JS_CODEGEN_PPC64)
+  // This gets built on the stack as part of exception returns. Because
+  // it goes right on top of the stack, an ABI-compliant routine can wreck
+  // it, so we implement a minimum Power ISA linkage area (four doublewords).
+  void *_ppc_sp_;
+  void *_ppc_cr_;
+  void *_ppc_lr_;
+  void *_ppc_toc_;
+#endif
+
   uint8_t* framePointer;
   uint8_t* stackPointer;
   uint8_t* target;
   ExceptionResumeKind kind;
   wasm::Instance* instance;
 
   // Value to push when resuming into a |finally| block.
   // Also used by Wasm to send the exception object to the throw stub.
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/LIR.h
--- a/js/src/jit/LIR.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/LIR.h	Thu Aug 08 21:25:14 2024 -0700
@@ -549,17 +549,17 @@
   static LDefinition BogusTemp() { return LDefinition(); }
 
   Policy policy() const {
     return (Policy)((bits_ >> POLICY_SHIFT) & POLICY_MASK);
   }
   Type type() const { return (Type)((bits_ >> TYPE_SHIFT) & TYPE_MASK); }
 
   static bool isFloatRegCompatible(Type type, FloatRegister reg) {
-#ifdef JS_CODEGEN_RISCV64
+#if defined(JS_CODEGEN_RISCV64) || defined(JS_CODEGEN_PPC64)
     if (type == FLOAT32 || type == DOUBLE) {
       return reg.isSingle() || reg.isDouble();
     }
 #else
     if (type == FLOAT32) {
       return reg.isSingle();
     }
     if (type == DOUBLE) {
@@ -2017,16 +2017,18 @@
 #  include "jit/riscv64/LIR-riscv64.h"
 #elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
 #  if defined(JS_CODEGEN_MIPS32)
 #    include "jit/mips32/LIR-mips32.h"
 #  elif defined(JS_CODEGEN_MIPS64)
 #    include "jit/mips64/LIR-mips64.h"
 #  endif
 #  include "jit/mips-shared/LIR-mips-shared.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/LIR-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/LIR-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/LIR-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/LIROps.yaml
--- a/js/src/jit/LIROps.yaml	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/LIROps.yaml	Thu Aug 08 21:25:14 2024 -0700
@@ -4405,16 +4405,57 @@
 
 - name: WasmAtomicBinopI64
   gen_boilerplate: false
 
 - name: WasmAtomicExchangeI64
   gen_boilerplate: false
 #endif
 
+#ifdef JS_CODEGEN_PPC64
+- name: DivOrModI64
+  gen_boilerplate: false
+
+- name: UDivOrMod
+  gen_boilerplate: false
+
+- name: UDivOrModI64
+  gen_boilerplate: false
+
+- name: ModMaskI
+  gen_boilerplate: false
+
+- name: WasmTruncateToInt64
+  gen_boilerplate: false
+
+- name: Int64ToFloatingPoint
+  gen_boilerplate: false
+
+- name: WasmUnalignedLoad
+  gen_boilerplate: false
+
+- name: WasmUnalignedLoadI64
+  gen_boilerplate: false
+
+- name: WasmUnalignedStore
+  gen_boilerplate: false
+
+- name: WasmUnalignedStoreI64
+  gen_boilerplate: false
+
+- name: WasmCompareExchangeI64
+  gen_boilerplate: false
+
+- name: WasmAtomicBinopI64
+  gen_boilerplate: false
+
+- name: WasmAtomicExchangeI64
+  gen_boilerplate: false
+#endif
+
 #ifdef FUZZING_JS_FUZZILLI
 - name: FuzzilliHashT
   gen_boilerplate: false
 
 - name: FuzzilliHashV
   gen_boilerplate: false
 
 - name: FuzzilliHashStore
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/Label.h
--- a/js/src/jit/Label.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/Label.h	Thu Aug 08 21:25:14 2024 -0700
@@ -22,17 +22,18 @@
 
   // offset_ < INVALID_OFFSET means that the label is either bound or has
   // incoming uses and needs to be bound.
   uint32_t offset_ : 31;
 
   void operator=(const LabelBase& label) = delete;
 
 #if defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) || \
-    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64) || \
+    defined(JS_CODEGEN_PPC64)
  public:
 #endif
   static const uint32_t INVALID_OFFSET = 0x7fffffff;  // UINT31_MAX.
 
  public:
   LabelBase() : bound_(false), offset_(INVALID_OFFSET) {}
 
   // If the label is bound, all incoming edges have been patched and any
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/Lowering.h
--- a/js/src/jit/Lowering.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/Lowering.h	Thu Aug 08 21:25:14 2024 -0700
@@ -22,16 +22,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/Lowering-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/Lowering-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/Lowering-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/Lowering-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/Lowering-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/Lowering-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/Lowering-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/MacroAssembler-inl.h
--- a/js/src/jit/MacroAssembler-inl.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/MacroAssembler-inl.h	Thu Aug 08 21:25:14 2024 -0700
@@ -38,16 +38,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/MacroAssembler-mips32-inl.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/MacroAssembler-mips64-inl.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/MacroAssembler-loong64-inl.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/MacroAssembler-riscv64-inl.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/MacroAssembler-ppc64-inl.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/MacroAssembler-wasm32-inl.h"
 #elif !defined(JS_CODEGEN_NONE)
 #  error "Unknown architecture!"
 #endif
 
 #include "wasm/WasmBuiltins.h"
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/MacroAssembler.cpp
--- a/js/src/jit/MacroAssembler.cpp	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/MacroAssembler.cpp	Thu Aug 08 21:25:14 2024 -0700
@@ -3783,17 +3783,18 @@
                                            wasm::BytecodeOffset callOffset) {
   if (compilingWasm) {
     Push(InstanceReg);
   }
   int32_t framePushedAfterInstance = framePushed();
 
 #if defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_ARM64) ||     \
     defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) || \
-    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64) || \
+    defined(JS_CODEGEN_PPC64)
   ScratchDoubleScope fpscratch(*this);
   if (widenFloatToDouble) {
     convertFloat32ToDouble(src, fpscratch);
     src = fpscratch;
   }
 #elif defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_X64)
   FloatRegister srcSingle;
   if (widenFloatToDouble) {
@@ -3822,17 +3823,18 @@
     passABIArg(src, ABIType::Float64);
     callWithABI<Fn, JS::ToInt32>(ABIType::General,
                                  CheckUnsafeCallWithABI::DontCheckOther);
   }
   storeCallInt32Result(dest);
 
 #if defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_ARM64) ||     \
     defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) || \
-    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_RISCV64) || \
+    defined(JS_CODEGEN_PPC64)
   // Nothing
 #elif defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_X64)
   if (widenFloatToDouble) {
     Pop(srcSingle);
   }
 #else
   MOZ_CRASH("MacroAssembler platform hook: outOfLineTruncateSlow");
 #endif
@@ -7411,16 +7413,18 @@
 #elif JS_CODEGEN_MIPS32
   ma_sll(temp1, temp1, temp3);
 #elif JS_CODEGEN_MIPS64
   ma_dsll(temp1, temp1, temp3);
 #elif JS_CODEGEN_LOONG64
   as_sll_d(temp1, temp1, temp3);
 #elif JS_CODEGEN_RISCV64
   sll(temp1, temp1, temp3);
+#elif JS_CODEGEN_PPC64
+  as_sld(temp1, temp1, temp3);
 #elif JS_CODEGEN_WASM32
   MOZ_CRASH();
 #elif JS_CODEGEN_NONE
   MOZ_CRASH();
 #else
 #  error "Unknown architecture"
 #endif
 
@@ -8872,16 +8876,25 @@
     assumeUnreachable("Unexpected BigInt");
   } else if (isBigInt == IsBigInt::Yes) {
     branchTestBigInt(Assembler::Equal, value, &ok);
     assumeUnreachable("Unexpected non-BigInt");
   }
   bind(&ok);
 #endif
 
+#if defined(JS_CODEGEN_PPC64)
+  // If this was preceded by a MoveGroup instruction, the hash may have been
+  // loaded algebraically since it's an Int32 (and thus sign-extended); the
+  // operation doesn't know to keep the upper bits clear, failing the assert.
+  if (isBigInt == IsBigInt::No) {
+    as_rldicl(hash, hash, 0, 32); // "clrldi"
+  }
+#endif
+
 #ifdef DEBUG
   PushRegsInMask(LiveRegisterSet(RegisterSet::Volatile()));
 
   pushValue(value);
   moveStackPtrTo(temp2);
 
   setupUnalignedABICall(temp1);
   loadJSContext(temp1);
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/MacroAssembler.h
--- a/js/src/jit/MacroAssembler.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/MacroAssembler.h	Thu Aug 08 21:25:14 2024 -0700
@@ -26,16 +26,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/MacroAssembler-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/MacroAssembler-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/MacroAssembler-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/MacroAssembler-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/MacroAssembler-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/MacroAssembler-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/MacroAssembler-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 #include "jit/ABIArgGenerator.h"
@@ -95,19 +97,20 @@
 //   //{{{ check_macroassembler_style
 //   inline uint32_t
 //   MacroAssembler::framePushed() const
 //   {
 //       return framePushed_;
 //   }
 //   ////}}} check_macroassembler_style
 
-#define ALL_ARCH mips32, mips64, arm, arm64, x86, x64, loong64, riscv64, wasm32
+#define ALL_ARCH mips32, mips64, arm, arm64, x86, x64, loong64, riscv64, \
+  ppc64, wasm32
 #define ALL_SHARED_ARCH \
-  arm, arm64, loong64, riscv64, x86_shared, mips_shared, wasm32
+  arm, arm64, loong64, riscv64, ppc64, x86_shared, mips_shared, wasm32
 
 // * How this macro works:
 //
 // DEFINED_ON is a macro which check if, for the current architecture, the
 // method is defined on the macro assembler or not.
 //
 // For each architecture, we have a macro named DEFINED_ON_arch.  This macro is
 // empty if this is not the current architecture.  Otherwise it must be either
@@ -145,16 +148,17 @@
 #define DEFINED_ON_x86_shared
 #define DEFINED_ON_arm
 #define DEFINED_ON_arm64
 #define DEFINED_ON_mips32
 #define DEFINED_ON_mips64
 #define DEFINED_ON_mips_shared
 #define DEFINED_ON_loong64
 #define DEFINED_ON_riscv64
+#define DEFINED_ON_ppc64
 #define DEFINED_ON_wasm32
 #define DEFINED_ON_none
 
 // Specialize for each architecture.
 #if defined(JS_CODEGEN_X86)
 #  undef DEFINED_ON_x86
 #  define DEFINED_ON_x86 define
 #  undef DEFINED_ON_x86_shared
@@ -181,16 +185,19 @@
 #  undef DEFINED_ON_mips_shared
 #  define DEFINED_ON_mips_shared define
 #elif defined(JS_CODEGEN_LOONG64)
 #  undef DEFINED_ON_loong64
 #  define DEFINED_ON_loong64 define
 #elif defined(JS_CODEGEN_RISCV64)
 #  undef DEFINED_ON_riscv64
 #  define DEFINED_ON_riscv64 define
+#elif defined(JS_CODEGEN_PPC64)
+#  undef DEFINED_ON_ppc64
+#  define DEFINED_ON_ppc64 define
 #elif defined(JS_CODEGEN_WASM32)
 #  undef DEFINED_ON_wasm32
 #  define DEFINED_ON_wasm32 define
 #elif defined(JS_CODEGEN_NONE)
 #  undef DEFINED_ON_none
 #  define DEFINED_ON_none crash
 #else
 #  error "Unknown architecture!"
@@ -524,39 +531,39 @@
   // the same layout to be used in machineState, and therefore in all other code
   // that can spill registers that are recovered on bailout. Implementations of
   // JitRuntime::generate{Invalidator,BailoutHandler} should either call
   // PushRegsInMask, or check carefully to be sure that they generate the same
   // layout.
 
   // The size of the area used by PushRegsInMask.
   static size_t PushRegsInMaskSizeInBytes(LiveRegisterSet set)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   void PushRegsInMask(LiveRegisterSet set)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
   void PushRegsInMask(LiveGeneralRegisterSet set);
 
   // Like PushRegsInMask, but instead of pushing the registers, store them to
   // |dest|. |dest| should point to the end of the reserved space, so the
   // first register will be stored at |dest.offset - sizeof(register)|.  It is
   // required that |dest.offset| is at least as large as the value computed by
   // PushRegsInMaskSizeInBytes for this |set|.  In other words, |dest.base|
   // must point to either the lowest address in the save area, or some address
   // below that.
   void storeRegsInMask(LiveRegisterSet set, Address dest, Register scratch)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   void PopRegsInMask(LiveRegisterSet set);
   void PopRegsInMask(LiveGeneralRegisterSet set);
   void PopRegsInMaskIgnore(LiveRegisterSet set, LiveRegisterSet ignore)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   // ===============================================================
   // Stack manipulation functions -- single registers/values.
 
   void Push(const Operand op) DEFINED_ON(x86_shared);
   void Push(Register reg) PER_SHARED_ARCH;
   void Push(Register reg1, Register reg2, Register reg3, Register reg4)
@@ -581,26 +588,26 @@
   inline CodeOffset PushWithPatch(ImmPtr imm);
 
   void Pop(const Operand op) DEFINED_ON(x86_shared);
   void Pop(Register reg) PER_SHARED_ARCH;
   void Pop(FloatRegister t) PER_SHARED_ARCH;
   void Pop(const ValueOperand& val) PER_SHARED_ARCH;
   void PopFlags() DEFINED_ON(x86_shared);
   void PopStackPtr()
-      DEFINED_ON(arm, mips_shared, x86_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, mips_shared, x86_shared, loong64, riscv64, ppc64, wasm32);
 
   // Move the stack pointer based on the requested amount.
   void adjustStack(int amount);
   void freeStack(uint32_t amount);
 
   // Move the stack pointer to the specified position. It assumes the SP
   // register is not valid -- it uses FP to set the position.
   void freeStackTo(uint32_t framePushed)
-      DEFINED_ON(x86_shared, arm, arm64, loong64, mips64, riscv64);
+      DEFINED_ON(x86_shared, arm, arm64, loong64, mips64, riscv64, ppc64);
 
   // Warning: This method does not update the framePushed() counter.
   void freeStack(Register amount);
 
  private:
   // ===============================================================
   // Register allocation fields.
 #ifdef DEBUG
@@ -643,19 +650,19 @@
   // Push the return address and make a call. On platforms where this function
   // is not defined, push the link register (pushReturnAddress) at the entry
   // point of the callee.
   void callAndPushReturnAddress(Register reg) DEFINED_ON(x86_shared);
   void callAndPushReturnAddress(Label* label) DEFINED_ON(x86_shared);
 
   // These do not adjust framePushed().
   void pushReturnAddress()
-      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, ppc64, wasm32);
   void popReturnAddress()
-      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, ppc64, wasm32);
 
   // Useful for dealing with two-valued returns.
   void moveRegPair(Register src0, Register src1, Register dst0, Register dst1,
                    MoveOp::Type type = MoveOp::GENERAL);
 
   void reserveVMFunctionOutParamSpace(const VMFunctionData& f);
   void loadVMFunctionOutParam(const VMFunctionData& f, const Address& addr);
 
@@ -681,17 +688,17 @@
   // using pc-relative addressing on certain platforms (RIP-relative LEA on x64,
   // ADR instruction on arm64).
   //
   // Note: "Near" applies to ARM64 where the target must be within 1 MB (this is
   // release-asserted).
   CodeOffset moveNearAddressWithPatch(Register dest) PER_ARCH;
   static void patchNearAddressMove(CodeLocationLabel loc,
                                    CodeLocationLabel target)
-      DEFINED_ON(x86, x64, arm, arm64, loong64, riscv64, wasm32, mips_shared);
+      DEFINED_ON(x86, x64, arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared);
 
  public:
   // ===============================================================
   // [SMDOC] JIT-to-C++ Function Calls (callWithABI)
   //
   // callWithABI is used to make a call using the standard C/C++ system ABI.
   //
   // callWithABI is a low level interface for making calls, as such every call
@@ -1066,21 +1073,21 @@
   inline void xor32(Imm32 imm, Register dest) PER_SHARED_ARCH;
   inline void xor32(Imm32 imm, const Address& dest) PER_SHARED_ARCH;
   inline void xor32(const Address& src, Register dest) PER_SHARED_ARCH;
 
   inline void xorPtr(Register src, Register dest) PER_ARCH;
   inline void xorPtr(Imm32 imm, Register dest) PER_ARCH;
 
   inline void and64(const Operand& src, Register64 dest)
-      DEFINED_ON(x64, mips64, loong64, riscv64);
+      DEFINED_ON(x64, mips64, loong64, riscv64, ppc64);
   inline void or64(const Operand& src, Register64 dest)
-      DEFINED_ON(x64, mips64, loong64, riscv64);
+      DEFINED_ON(x64, mips64, loong64, riscv64, ppc64);
   inline void xor64(const Operand& src, Register64 dest)
-      DEFINED_ON(x64, mips64, loong64, riscv64);
+      DEFINED_ON(x64, mips64, loong64, riscv64, ppc64);
 
   // ===============================================================
   // Swap instructions
 
   // Swap the two lower bytes and sign extend the result to 32-bit.
   inline void byteSwap16SignExtend(Register reg) PER_SHARED_ARCH;
 
   // Swap the two lower bytes and zero extend the result to 32-bit.
@@ -1110,27 +1117,27 @@
   inline void addPtr(Register src, Register dest) PER_ARCH;
   inline void addPtr(Register src1, Register src2, Register dest)
       DEFINED_ON(arm64);
   inline void addPtr(Imm32 imm, Register dest) PER_ARCH;
   inline void addPtr(Imm32 imm, Register src, Register dest) DEFINED_ON(arm64);
   inline void addPtr(ImmWord imm, Register dest) PER_ARCH;
   inline void addPtr(ImmPtr imm, Register dest);
   inline void addPtr(Imm32 imm, const Address& dest)
-      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void addPtr(Imm32 imm, const AbsoluteAddress& dest)
       DEFINED_ON(x86, x64);
   inline void addPtr(const Address& src, Register dest)
-      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   inline void add64(Register64 src, Register64 dest) PER_ARCH;
   inline void add64(Imm32 imm, Register64 dest) PER_ARCH;
   inline void add64(Imm64 imm, Register64 dest) PER_ARCH;
   inline void add64(const Operand& src, Register64 dest)
-      DEFINED_ON(x64, mips64, loong64, riscv64);
+      DEFINED_ON(x64, mips64, loong64, riscv64, ppc64);
 
   inline void addFloat32(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
 
   // Compute dest=SP-imm where dest is a pointer registers and not SP.  The
   // offset returned from sub32FromStackPtrWithPatch() must be passed to
   // patchSub32FromStackPtr().
   inline CodeOffset sub32FromStackPtrWithPatch(Register dest) PER_ARCH;
   inline void patchSub32FromStackPtr(CodeOffset offset, Imm32 imm) PER_ARCH;
@@ -1139,26 +1146,26 @@
   inline void addConstantDouble(double d, FloatRegister dest) DEFINED_ON(x86);
 
   inline void sub32(const Address& src, Register dest) PER_SHARED_ARCH;
   inline void sub32(Register src, Register dest) PER_SHARED_ARCH;
   inline void sub32(Imm32 imm, Register dest) PER_SHARED_ARCH;
 
   inline void subPtr(Register src, Register dest) PER_ARCH;
   inline void subPtr(Register src, const Address& dest)
-      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void subPtr(Imm32 imm, Register dest) PER_ARCH;
   inline void subPtr(ImmWord imm, Register dest) DEFINED_ON(x64);
   inline void subPtr(const Address& addr, Register dest)
-      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   inline void sub64(Register64 src, Register64 dest) PER_ARCH;
   inline void sub64(Imm64 imm, Register64 dest) PER_ARCH;
   inline void sub64(const Operand& src, Register64 dest)
-      DEFINED_ON(x64, mips64, loong64, riscv64);
+      DEFINED_ON(x64, mips64, loong64, riscv64, ppc64);
 
   inline void subFloat32(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
 
   inline void subDouble(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
 
   inline void mul32(Register rhs, Register srcDest) PER_SHARED_ARCH;
   inline void mul32(Imm32 imm, Register srcDest) PER_SHARED_ARCH;
 
@@ -1169,91 +1176,91 @@
   inline void mulHighUnsigned32(Imm32 imm, Register src,
                                 Register dest) PER_ARCH;
 
   inline void mulPtr(Register rhs, Register srcDest) PER_ARCH;
 
   inline void mul64(const Operand& src, const Register64& dest) DEFINED_ON(x64);
   inline void mul64(const Operand& src, const Register64& dest,
                     const Register temp)
-      DEFINED_ON(x64, mips64, loong64, riscv64);
+      DEFINED_ON(x64, mips64, loong64, riscv64, ppc64);
   inline void mul64(Imm64 imm, const Register64& dest) PER_ARCH;
   inline void mul64(Imm64 imm, const Register64& dest, const Register temp)
-      DEFINED_ON(x86, x64, arm, mips32, mips64, loong64, riscv64);
+      DEFINED_ON(x86, x64, arm, mips32, mips64, loong64, riscv64, ppc64);
   inline void mul64(const Register64& src, const Register64& dest,
                     const Register temp) PER_ARCH;
   inline void mul64(const Register64& src1, const Register64& src2,
                     const Register64& dest) DEFINED_ON(arm64);
   inline void mul64(Imm64 src1, const Register64& src2, const Register64& dest)
       DEFINED_ON(arm64);
 
   inline void mulBy3(Register src, Register dest) PER_ARCH;
 
   inline void mulFloat32(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
   inline void mulDouble(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
 
   inline void mulDoublePtr(ImmPtr imm, Register temp, FloatRegister dest)
-      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   // Perform an integer division, returning the integer part rounded toward
   // zero. rhs must not be zero, and the division must not overflow.
   //
   // On ARM, the chip must have hardware division instructions.
   inline void quotient32(Register rhs, Register srcDest, bool isUnsigned)
-      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, ppc64, wasm32);
 
   // As above, but srcDest must be eax and tempEdx must be edx.
   inline void quotient32(Register rhs, Register srcDest, Register tempEdx,
                          bool isUnsigned) DEFINED_ON(x86_shared);
 
   // Perform an integer division, returning the remainder part.
   // rhs must not be zero, and the division must not overflow.
   //
   // On ARM, the chip must have hardware division instructions.
   inline void remainder32(Register rhs, Register srcDest, bool isUnsigned)
-      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, loong64, riscv64, ppc64, wasm32);
 
   // As above, but srcDest must be eax and tempEdx must be edx.
   inline void remainder32(Register rhs, Register srcDest, Register tempEdx,
                           bool isUnsigned) DEFINED_ON(x86_shared);
 
   // Perform an integer division, returning the integer part rounded toward
   // zero. rhs must not be zero, and the division must not overflow.
   //
   // This variant preserves registers, and doesn't require hardware division
   // instructions on ARM (will call out to a runtime routine).
   //
   // rhs is preserved, srdDest is clobbered.
   void flexibleRemainder32(Register rhs, Register srcDest, bool isUnsigned,
                            const LiveRegisterSet& volatileLiveRegs)
-      DEFINED_ON(mips_shared, arm, arm64, x86_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86_shared, loong64, riscv64, ppc64, wasm32);
 
   // Perform an integer division, returning the integer part rounded toward
   // zero. rhs must not be zero, and the division must not overflow.
   //
   // This variant preserves registers, and doesn't require hardware division
   // instructions on ARM (will call out to a runtime routine).
   //
   // rhs is preserved, srdDest is clobbered.
   void flexibleQuotient32(Register rhs, Register srcDest, bool isUnsigned,
                           const LiveRegisterSet& volatileLiveRegs)
-      DEFINED_ON(mips_shared, arm, arm64, x86_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, arm, arm64, x86_shared, loong64, riscv64, ppc64);
 
   // Perform an integer division, returning the integer part rounded toward
   // zero. rhs must not be zero, and the division must not overflow. The
   // remainder is stored into the third argument register here.
   //
   // This variant preserves registers, and doesn't require hardware division
   // instructions on ARM (will call out to a runtime routine).
   //
   // rhs is preserved, srdDest and remOutput are clobbered.
   void flexibleDivMod32(Register rhs, Register srcDest, Register remOutput,
                         bool isUnsigned,
                         const LiveRegisterSet& volatileLiveRegs)
-      DEFINED_ON(mips_shared, arm, arm64, x86_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(mips_shared, arm, arm64, x86_shared, loong64, riscv64, ppc64, wasm32);
 
   inline void divFloat32(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
   inline void divDouble(FloatRegister src, FloatRegister dest) PER_SHARED_ARCH;
 
   inline void inc64(AbsoluteAddress dest) PER_ARCH;
 
   inline void neg32(Register reg) PER_SHARED_ARCH;
   inline void neg64(Register64 reg) PER_ARCH;
@@ -1461,17 +1468,17 @@
                       Register dest) PER_SHARED_ARCH;
 
   inline void cmp16Set(Condition cond, Address lhs, Imm32 rhs,
                        Register dest) PER_SHARED_ARCH;
 
   template <typename T1, typename T2>
   inline void cmp32Set(Condition cond, T1 lhs, T2 rhs, Register dest)
       DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64,
-                 wasm32);
+                 ppc64, wasm32);
 
   // Only the NotEqual and Equal conditions are allowed.
   inline void cmp64Set(Condition cond, Address lhs, Imm64 rhs,
                        Register dest) PER_ARCH;
 
   template <typename T1, typename T2>
   inline void cmpPtrSet(Condition cond, T1 lhs, T2 rhs, Register dest) PER_ARCH;
 
@@ -1503,34 +1510,34 @@
 
   inline void branch32(Condition cond, const Address& lhs, Register rhs,
                        Label* label) PER_SHARED_ARCH;
   inline void branch32(Condition cond, const Address& lhs, Imm32 rhs,
                        Label* label) PER_SHARED_ARCH;
 
   inline void branch32(Condition cond, const AbsoluteAddress& lhs, Register rhs,
                        Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void branch32(Condition cond, const AbsoluteAddress& lhs, Imm32 rhs,
                        Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   inline void branch32(Condition cond, const BaseIndex& lhs, Register rhs,
                        Label* label) DEFINED_ON(arm, x86_shared);
   inline void branch32(Condition cond, const BaseIndex& lhs, Imm32 rhs,
                        Label* label) PER_SHARED_ARCH;
 
   inline void branch32(Condition cond, const Operand& lhs, Register rhs,
                        Label* label) DEFINED_ON(x86_shared);
   inline void branch32(Condition cond, const Operand& lhs, Imm32 rhs,
                        Label* label) DEFINED_ON(x86_shared);
 
   inline void branch32(Condition cond, wasm::SymbolicAddress lhs, Imm32 rhs,
                        Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   // The supported condition are Equal, NotEqual, LessThan(orEqual),
   // GreaterThan(orEqual), Below(orEqual) and Above(orEqual). When a fail label
   // is not defined it will fall through to next instruction, else jump to the
   // fail label.
   inline void branch64(Condition cond, Register64 lhs, Imm64 val,
                        Label* success, Label* fail = nullptr) PER_ARCH;
   inline void branch64(Condition cond, Register64 lhs, Register64 rhs,
@@ -1571,32 +1578,32 @@
 
   inline void branchPtr(Condition cond, const BaseIndex& lhs, ImmWord rhs,
                         Label* label) PER_SHARED_ARCH;
   inline void branchPtr(Condition cond, const BaseIndex& lhs, Register rhs,
                         Label* label) PER_SHARED_ARCH;
 
   inline void branchPtr(Condition cond, const AbsoluteAddress& lhs,
                         Register rhs, Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void branchPtr(Condition cond, const AbsoluteAddress& lhs, ImmWord rhs,
                         Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   inline void branchPtr(Condition cond, wasm::SymbolicAddress lhs, Register rhs,
                         Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   // Given a pointer to a GC Cell, retrieve the StoreBuffer pointer from its
   // chunk header, or nullptr if it is in the tenured heap.
   void loadStoreBuffer(Register ptr, Register buffer) PER_ARCH;
 
   void branchPtrInNurseryChunk(Condition cond, Register ptr, Register temp,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   void branchPtrInNurseryChunk(Condition cond, const Address& address,
                                Register temp, Label* label) DEFINED_ON(x86);
   void branchValueIsNurseryCell(Condition cond, const Address& address,
                                 Register temp, Label* label) PER_ARCH;
   void branchValueIsNurseryCell(Condition cond, ValueOperand value,
                                 Register temp, Label* label) PER_ARCH;
 
   // This function compares a Value (lhs) which is having a private pointer
@@ -1608,33 +1615,33 @@
                           FloatRegister rhs, Label* label) PER_SHARED_ARCH;
 
   // Truncate a double/float32 to int32 and when it doesn't fit an int32 it will
   // jump to the failure label. This particular variant is allowed to return the
   // value module 2**32, which isn't implemented on all architectures. E.g. the
   // x64 variants will do this only in the int64_t range.
   inline void branchTruncateFloat32MaybeModUint32(FloatRegister src,
                                                   Register dest, Label* fail)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void branchTruncateDoubleMaybeModUint32(FloatRegister src,
                                                  Register dest, Label* fail)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   // Truncate a double/float32 to intptr and when it doesn't fit jump to the
   // failure label.
   inline void branchTruncateFloat32ToPtr(FloatRegister src, Register dest,
                                          Label* fail) DEFINED_ON(x86, x64);
   inline void branchTruncateDoubleToPtr(FloatRegister src, Register dest,
                                         Label* fail) DEFINED_ON(x86, x64);
 
   // Truncate a double/float32 to int32 and when it doesn't fit jump to the
   // failure label.
   inline void branchTruncateFloat32ToInt32(FloatRegister src, Register dest,
                                            Label* fail)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void branchTruncateDoubleToInt32(FloatRegister src, Register dest,
                                           Label* fail) PER_ARCH;
 
   inline void branchDouble(DoubleCondition cond, FloatRegister lhs,
                            FloatRegister rhs, Label* label) PER_SHARED_ARCH;
 
   inline void branchDoubleNotInInt64Range(Address src, Register temp,
                                           Label* fail);
@@ -1683,17 +1690,17 @@
                            L label) PER_SHARED_ARCH;
   template <class L>
   inline void branchTest32(Condition cond, Register lhs, Imm32 rhs,
                            L label) PER_SHARED_ARCH;
   inline void branchTest32(Condition cond, const Address& lhs, Imm32 rhh,
                            Label* label) PER_SHARED_ARCH;
   inline void branchTest32(Condition cond, const AbsoluteAddress& lhs,
                            Imm32 rhs, Label* label)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   template <class L>
   inline void branchTestPtr(Condition cond, Register lhs, Register rhs,
                             L label) PER_SHARED_ARCH;
   inline void branchTestPtr(Condition cond, Register lhs, Imm32 rhs,
                             Label* label) PER_SHARED_ARCH;
   inline void branchTestPtr(Condition cond, const Address& lhs, Imm32 rhs,
                             Label* label) PER_SHARED_ARCH;
@@ -1867,17 +1874,17 @@
 
   // Perform a type-test on a tag of a Value (32bits boxing), or the tagged
   // value (64bits boxing).
   inline void branchTestUndefined(Condition cond, Register tag,
                                   Label* label) PER_SHARED_ARCH;
   inline void branchTestInt32(Condition cond, Register tag,
                               Label* label) PER_SHARED_ARCH;
   inline void branchTestDouble(Condition cond, Register tag, Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
   inline void branchTestNumber(Condition cond, Register tag,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestBoolean(Condition cond, Register tag,
                                 Label* label) PER_SHARED_ARCH;
   inline void branchTestString(Condition cond, Register tag,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestSymbol(Condition cond, Register tag,
@@ -1900,117 +1907,117 @@
   // BaseIndex and ValueOperand variants clobber the ScratchReg on x64.
   // All Variants clobber the ScratchReg on arm64.
   inline void branchTestUndefined(Condition cond, const Address& address,
                                   Label* label) PER_SHARED_ARCH;
   inline void branchTestUndefined(Condition cond, const BaseIndex& address,
                                   Label* label) PER_SHARED_ARCH;
   inline void branchTestUndefined(Condition cond, const ValueOperand& value,
                                   Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestInt32(Condition cond, const Address& address,
                               Label* label) PER_SHARED_ARCH;
   inline void branchTestInt32(Condition cond, const BaseIndex& address,
                               Label* label) PER_SHARED_ARCH;
   inline void branchTestInt32(Condition cond, const ValueOperand& value,
                               Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestDouble(Condition cond, const Address& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestDouble(Condition cond, const BaseIndex& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestDouble(Condition cond, const ValueOperand& value,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestNumber(Condition cond, const ValueOperand& value,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestBoolean(Condition cond, const Address& address,
                                 Label* label) PER_SHARED_ARCH;
   inline void branchTestBoolean(Condition cond, const BaseIndex& address,
                                 Label* label) PER_SHARED_ARCH;
   inline void branchTestBoolean(Condition cond, const ValueOperand& value,
                                 Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestString(Condition cond, const Address& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestString(Condition cond, const BaseIndex& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestString(Condition cond, const ValueOperand& value,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestSymbol(Condition cond, const Address& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestSymbol(Condition cond, const BaseIndex& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestSymbol(Condition cond, const ValueOperand& value,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestBigInt(Condition cond, const Address& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestBigInt(Condition cond, const BaseIndex& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestBigInt(Condition cond, const ValueOperand& value,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestNull(Condition cond, const Address& address,
                              Label* label) PER_SHARED_ARCH;
   inline void branchTestNull(Condition cond, const BaseIndex& address,
                              Label* label) PER_SHARED_ARCH;
   inline void branchTestNull(Condition cond, const ValueOperand& value,
                              Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   // Clobbers the ScratchReg on x64.
   inline void branchTestObject(Condition cond, const Address& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestObject(Condition cond, const BaseIndex& address,
                                Label* label) PER_SHARED_ARCH;
   inline void branchTestObject(Condition cond, const ValueOperand& value,
                                Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestGCThing(Condition cond, const Address& address,
                                 Label* label) PER_SHARED_ARCH;
   inline void branchTestGCThing(Condition cond, const BaseIndex& address,
                                 Label* label) PER_SHARED_ARCH;
   inline void branchTestGCThing(Condition cond, const ValueOperand& value,
                                 Label* label) PER_SHARED_ARCH;
 
   inline void branchTestPrimitive(Condition cond, const ValueOperand& value,
                                   Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestMagic(Condition cond, const Address& address,
                               Label* label) PER_SHARED_ARCH;
   inline void branchTestMagic(Condition cond, const BaseIndex& address,
                               Label* label) PER_SHARED_ARCH;
   template <class L>
   inline void branchTestMagic(Condition cond, const ValueOperand& value,
                               L label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   inline void branchTestMagic(Condition cond, const Address& valaddr,
                               JSWhyMagic why, Label* label) PER_ARCH;
 
   inline void branchTestMagicValue(Condition cond, const ValueOperand& val,
                                    JSWhyMagic why, Label* label);
 
@@ -2019,45 +2026,45 @@
 
   inline void branchTestValue(Condition cond, const BaseIndex& lhs,
                               const ValueOperand& rhs, Label* label) PER_ARCH;
 
   // Checks if given Value is evaluated to true or false in a condition.
   // The type of the value should match the type of the method.
   inline void branchTestInt32Truthy(bool truthy, const ValueOperand& value,
                                     Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, x86_shared,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, x86_shared,
                  wasm32);
   inline void branchTestDoubleTruthy(bool truthy, FloatRegister reg,
                                      Label* label) PER_SHARED_ARCH;
   inline void branchTestBooleanTruthy(bool truthy, const ValueOperand& value,
                                       Label* label) PER_ARCH;
   inline void branchTestStringTruthy(bool truthy, const ValueOperand& value,
                                      Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
   inline void branchTestBigIntTruthy(bool truthy, const ValueOperand& value,
                                      Label* label)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, wasm32,
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, wasm32,
                  x86_shared);
 
   // Create an unconditional branch to the address given as argument.
   inline void branchToComputedAddress(const BaseIndex& address) PER_ARCH;
 
  private:
   template <typename T, typename S, typename L>
   inline void branchPtrImpl(Condition cond, const T& lhs, const S& rhs, L label)
       DEFINED_ON(x86_shared);
 
   void branchPtrInNurseryChunkImpl(Condition cond, Register ptr, Label* label)
       DEFINED_ON(x86);
   template <typename T>
   void branchValueIsNurseryCellImpl(Condition cond, const T& value,
                                     Register temp, Label* label)
-      DEFINED_ON(arm64, x64, mips64, loong64, riscv64);
+      DEFINED_ON(arm64, x64, mips64, loong64, riscv64, ppc64);
 
   template <typename T>
   inline void branchTestUndefinedImpl(Condition cond, const T& t, Label* label)
       DEFINED_ON(arm, arm64, x86_shared);
   template <typename T>
   inline void branchTestInt32Impl(Condition cond, const T& t, Label* label)
       DEFINED_ON(arm, arm64, x86_shared);
   template <typename T>
@@ -2134,129 +2141,129 @@
   inline void fallibleUnboxString(const T& src, Register dest, Label* fail);
   template <typename T>
   inline void fallibleUnboxSymbol(const T& src, Register dest, Label* fail);
   template <typename T>
   inline void fallibleUnboxBigInt(const T& src, Register dest, Label* fail);
 
   inline void cmp32Move32(Condition cond, Register lhs, Imm32 rhs, Register src,
                           Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86_shared);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86_shared);
 
   inline void cmp32Move32(Condition cond, Register lhs, Register rhs,
                           Register src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86_shared);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86_shared);
 
   inline void cmp32Move32(Condition cond, Register lhs, const Address& rhs,
                           Register src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86_shared);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86_shared);
 
   inline void cmpPtrMovePtr(Condition cond, Register lhs, Register rhs,
                             Register src, Register dest) PER_ARCH;
 
   inline void cmpPtrMovePtr(Condition cond, Register lhs, const Address& rhs,
                             Register src, Register dest) PER_ARCH;
 
   inline void cmp32Load32(Condition cond, Register lhs, const Address& rhs,
                           const Address& src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, mips_shared, x86_shared);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, mips_shared, x86_shared);
 
   inline void cmp32Load32(Condition cond, Register lhs, Register rhs,
                           const Address& src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, mips_shared, x86_shared);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, mips_shared, x86_shared);
 
   inline void cmp32Load32(Condition cond, Register lhs, Imm32 rhs,
                           const Address& src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86_shared);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86_shared);
 
   inline void cmp32LoadPtr(Condition cond, const Address& lhs, Imm32 rhs,
                            const Address& src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86, x64);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86, x64);
 
   inline void cmp32MovePtr(Condition cond, Register lhs, Imm32 rhs,
                            Register src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86, x64);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86, x64);
 
   inline void test32LoadPtr(Condition cond, const Address& addr, Imm32 mask,
                             const Address& src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86, x64);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86, x64);
 
   inline void test32MovePtr(Condition cond, const Address& addr, Imm32 mask,
                             Register src, Register dest)
-      DEFINED_ON(arm, arm64, loong64, riscv64, wasm32, mips_shared, x86, x64);
+      DEFINED_ON(arm, arm64, loong64, riscv64, ppc64, wasm32, mips_shared, x86, x64);
 
   // Conditional move for Spectre mitigations.
   inline void spectreMovePtr(Condition cond, Register src, Register dest)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   // Zeroes dest if the condition is true.
   inline void spectreZeroRegister(Condition cond, Register scratch,
                                   Register dest)
-      DEFINED_ON(arm, arm64, mips_shared, x86_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86_shared, loong64, riscv64, ppc64, wasm32);
 
   // Performs a bounds check and zeroes the index register if out-of-bounds
   // (to mitigate Spectre).
  private:
   inline void spectreBoundsCheck32(Register index, const Operand& length,
                                    Register maybeScratch, Label* failure)
       DEFINED_ON(x86);
 
  public:
   inline void spectreBoundsCheck32(Register index, Register length,
                                    Register maybeScratch, Label* failure)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void spectreBoundsCheck32(Register index, const Address& length,
                                    Register maybeScratch, Label* failure)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   inline void spectreBoundsCheckPtr(Register index, Register length,
                                     Register maybeScratch, Label* failure)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
   inline void spectreBoundsCheckPtr(Register index, const Address& length,
                                     Register maybeScratch, Label* failure)
-      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, mips_shared, x86, x64, loong64, riscv64, ppc64, wasm32);
 
   // ========================================================================
   // Canonicalization primitives.
   inline void canonicalizeDouble(FloatRegister reg);
   inline void canonicalizeDoubleIfDeterministic(FloatRegister reg);
 
   inline void canonicalizeFloat(FloatRegister reg);
   inline void canonicalizeFloatIfDeterministic(FloatRegister reg);
 
  public:
   // ========================================================================
   // Memory access primitives.
   inline FaultingCodeOffset storeUncanonicalizedDouble(FloatRegister src,
                                                        const Address& dest)
-      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64,
+      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64, ppc64,
                  wasm32);
   inline FaultingCodeOffset storeUncanonicalizedDouble(FloatRegister src,
                                                        const BaseIndex& dest)
-      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64,
+      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64, ppc64,
                  wasm32);
   inline FaultingCodeOffset storeUncanonicalizedDouble(FloatRegister src,
                                                        const Operand& dest)
       DEFINED_ON(x86_shared);
 
   template <class T>
   inline FaultingCodeOffset storeDouble(FloatRegister src, const T& dest);
 
   template <class T>
   inline void boxDouble(FloatRegister src, const T& dest);
 
   using MacroAssemblerSpecific::boxDouble;
 
   inline FaultingCodeOffset storeUncanonicalizedFloat32(FloatRegister src,
                                                         const Address& dest)
-      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64,
+      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64, ppc64,
                  wasm32);
   inline FaultingCodeOffset storeUncanonicalizedFloat32(FloatRegister src,
                                                         const BaseIndex& dest)
-      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64,
+      DEFINED_ON(x86_shared, arm, arm64, mips32, mips64, loong64, riscv64, ppc64,
                  wasm32);
   inline FaultingCodeOffset storeUncanonicalizedFloat32(FloatRegister src,
                                                         const Operand& dest)
       DEFINED_ON(x86_shared);
 
   template <class T>
   inline FaultingCodeOffset storeFloat32(FloatRegister src, const T& dest);
 
@@ -3669,20 +3676,20 @@
       DEFINED_ON(x86, x64);
 
  public:
   // ========================================================================
   // Convert floating point.
 
   // temp required on x86 and x64; must be undefined on mips64 and loong64.
   void convertUInt64ToFloat32(Register64 src, FloatRegister dest, Register temp)
-      DEFINED_ON(arm64, mips64, loong64, riscv64, wasm32, x64, x86);
+      DEFINED_ON(arm64, mips64, loong64, riscv64, ppc64, wasm32, x64, x86);
 
   void convertInt64ToFloat32(Register64 src, FloatRegister dest)
-      DEFINED_ON(arm64, mips64, loong64, riscv64, wasm32, x64, x86);
+      DEFINED_ON(arm64, mips64, loong64, riscv64, ppc64, wasm32, x64, x86);
 
   bool convertUInt64ToDoubleNeedsTemp() PER_ARCH;
 
   // temp required when convertUInt64ToDoubleNeedsTemp() returns true.
   void convertUInt64ToDouble(Register64 src, FloatRegister dest,
                              Register temp) PER_ARCH;
 
   void convertInt64ToDouble(Register64 src, FloatRegister dest) PER_ARCH;
@@ -3725,30 +3732,30 @@
   // On 32-bit systems for both wasm and asm.js, and on 64-bit systems for
   // asm.js, heap lengths are limited to 2GB.  On 64-bit systems for wasm,
   // 32-bit heap lengths are limited to 4GB, and 64-bit heap lengths will be
   // limited to something much larger.
 
   void wasmBoundsCheck32(Condition cond, Register index,
                          Register boundsCheckLimit, Label* ok)
       DEFINED_ON(arm, arm64, mips32, mips64, x86_shared, loong64, riscv64,
-                 wasm32);
+                 ppc64, wasm32);
 
   void wasmBoundsCheck32(Condition cond, Register index,
                          Address boundsCheckLimit, Label* ok)
       DEFINED_ON(arm, arm64, mips32, mips64, x86_shared, loong64, riscv64,
-                 wasm32);
+                 ppc64, wasm32);
 
   void wasmBoundsCheck64(Condition cond, Register64 index,
                          Register64 boundsCheckLimit, Label* ok)
-      DEFINED_ON(arm64, mips64, x64, x86, arm, loong64, riscv64, wasm32);
+      DEFINED_ON(arm64, mips64, x64, x86, arm, loong64, riscv64, ppc64, wasm32);
 
   void wasmBoundsCheck64(Condition cond, Register64 index,
                          Address boundsCheckLimit, Label* ok)
-      DEFINED_ON(arm64, mips64, x64, x86, arm, loong64, riscv64, wasm32);
+      DEFINED_ON(arm64, mips64, x64, x86, arm, loong64, riscv64, ppc64, wasm32);
 
   // Each wasm load/store instruction appends its own wasm::Trap::OutOfBounds.
   void wasmLoad(const wasm::MemoryAccessDesc& access, Operand srcAddr,
                 AnyRegister out) DEFINED_ON(x86, x64);
   void wasmLoadI64(const wasm::MemoryAccessDesc& access, Operand srcAddr,
                    Register64 out) DEFINED_ON(x86, x64);
   void wasmStore(const wasm::MemoryAccessDesc& access, AnyRegister value,
                  Operand dstAddr) DEFINED_ON(x86, x64);
@@ -3758,26 +3765,26 @@
   // For all the ARM/MIPS/LOONG64 wasmLoad and wasmStore functions below, `ptr`
   // MUST equal `ptrScratch`, and that register will be updated based on
   // conditions listed below (where it is only mentioned as `ptr`).
 
   // `ptr` will be updated if access.offset() != 0 or access.type() ==
   // Scalar::Int64.
   void wasmLoad(const wasm::MemoryAccessDesc& access, Register memoryBase,
                 Register ptr, Register ptrScratch, AnyRegister output)
-      DEFINED_ON(arm, loong64, riscv64, mips_shared);
+      DEFINED_ON(arm, loong64, riscv64, ppc64, mips_shared);
   void wasmLoadI64(const wasm::MemoryAccessDesc& access, Register memoryBase,
                    Register ptr, Register ptrScratch, Register64 output)
-      DEFINED_ON(arm, mips32, mips64, loong64, riscv64);
+      DEFINED_ON(arm, mips32, mips64, loong64, riscv64, ppc64);
   void wasmStore(const wasm::MemoryAccessDesc& access, AnyRegister value,
                  Register memoryBase, Register ptr, Register ptrScratch)
-      DEFINED_ON(arm, loong64, riscv64, mips_shared);
+      DEFINED_ON(arm, loong64, riscv64, ppc64, mips_shared);
   void wasmStoreI64(const wasm::MemoryAccessDesc& access, Register64 value,
                     Register memoryBase, Register ptr, Register ptrScratch)
-      DEFINED_ON(arm, mips32, mips64, loong64, riscv64);
+      DEFINED_ON(arm, mips32, mips64, loong64, riscv64, ppc64);
 
   // These accept general memoryBase + ptr + offset (in `access`); the offset is
   // always smaller than the guard region.  They will insert an additional add
   // if the offset is nonzero, and of course that add may require a temporary
   // register for the offset if the offset is large, and instructions to set it
   // up.
   void wasmLoad(const wasm::MemoryAccessDesc& access, Register memoryBase,
                 Register ptr, AnyRegister output) DEFINED_ON(arm64);
@@ -3787,99 +3794,99 @@
                  Register memoryBase, Register ptr) DEFINED_ON(arm64);
   void wasmStoreI64(const wasm::MemoryAccessDesc& access, Register64 value,
                     Register memoryBase, Register ptr) DEFINED_ON(arm64);
 
   // `ptr` will always be updated.
   void wasmUnalignedLoad(const wasm::MemoryAccessDesc& access,
                          Register memoryBase, Register ptr, Register ptrScratch,
                          Register output, Register tmp)
-      DEFINED_ON(mips32, mips64);
+      DEFINED_ON(mips32, mips64, ppc64);
 
   // MIPS: `ptr` will always be updated.
   void wasmUnalignedLoadFP(const wasm::MemoryAccessDesc& access,
                            Register memoryBase, Register ptr,
                            Register ptrScratch, FloatRegister output,
-                           Register tmp1) DEFINED_ON(mips32, mips64);
+                           Register tmp1) DEFINED_ON(mips32, mips64, ppc64);
 
   // `ptr` will always be updated.
   void wasmUnalignedLoadI64(const wasm::MemoryAccessDesc& access,
                             Register memoryBase, Register ptr,
                             Register ptrScratch, Register64 output,
-                            Register tmp) DEFINED_ON(mips32, mips64);
+                            Register tmp) DEFINED_ON(mips32, mips64, ppc64);
 
   // MIPS: `ptr` will always be updated.
   void wasmUnalignedStore(const wasm::MemoryAccessDesc& access, Register value,
                           Register memoryBase, Register ptr,
                           Register ptrScratch, Register tmp)
-      DEFINED_ON(mips32, mips64);
+      DEFINED_ON(mips32, mips64, ppc64);
 
   // `ptr` will always be updated.
   void wasmUnalignedStoreFP(const wasm::MemoryAccessDesc& access,
                             FloatRegister floatValue, Register memoryBase,
                             Register ptr, Register ptrScratch, Register tmp)
-      DEFINED_ON(mips32, mips64);
+      DEFINED_ON(mips32, mips64, ppc64);
 
   // `ptr` will always be updated.
   void wasmUnalignedStoreI64(const wasm::MemoryAccessDesc& access,
                              Register64 value, Register memoryBase,
                              Register ptr, Register ptrScratch, Register tmp)
-      DEFINED_ON(mips32, mips64);
+      DEFINED_ON(mips32, mips64, ppc64);
 
   // wasm specific methods, used in both the wasm baseline compiler and ion.
 
   // The truncate-to-int32 methods do not bind the rejoin label; clients must
   // do so if oolWasmTruncateCheckF64ToI32() can jump to it.
   void wasmTruncateDoubleToUInt32(FloatRegister input, Register output,
                                   bool isSaturating, Label* oolEntry) PER_ARCH;
   void wasmTruncateDoubleToInt32(FloatRegister input, Register output,
                                  bool isSaturating,
                                  Label* oolEntry) PER_SHARED_ARCH;
   void oolWasmTruncateCheckF64ToI32(FloatRegister input, Register output,
                                     TruncFlags flags, wasm::BytecodeOffset off,
                                     Label* rejoin)
-      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, ppc64, wasm32);
 
   void wasmTruncateFloat32ToUInt32(FloatRegister input, Register output,
                                    bool isSaturating, Label* oolEntry) PER_ARCH;
   void wasmTruncateFloat32ToInt32(FloatRegister input, Register output,
                                   bool isSaturating,
                                   Label* oolEntry) PER_SHARED_ARCH;
   void oolWasmTruncateCheckF32ToI32(FloatRegister input, Register output,
                                     TruncFlags flags, wasm::BytecodeOffset off,
                                     Label* rejoin)
-      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, ppc64, wasm32);
 
   // The truncate-to-int64 methods will always bind the `oolRejoin` label
   // after the last emitted instruction.
   void wasmTruncateDoubleToInt64(FloatRegister input, Register64 output,
                                  bool isSaturating, Label* oolEntry,
                                  Label* oolRejoin, FloatRegister tempDouble)
-      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, ppc64, wasm32);
   void wasmTruncateDoubleToUInt64(FloatRegister input, Register64 output,
                                   bool isSaturating, Label* oolEntry,
                                   Label* oolRejoin, FloatRegister tempDouble)
-      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, ppc64, wasm32);
   void oolWasmTruncateCheckF64ToI64(FloatRegister input, Register64 output,
                                     TruncFlags flags, wasm::BytecodeOffset off,
                                     Label* rejoin)
-      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, ppc64, wasm32);
 
   void wasmTruncateFloat32ToInt64(FloatRegister input, Register64 output,
                                   bool isSaturating, Label* oolEntry,
                                   Label* oolRejoin, FloatRegister tempDouble)
-      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, ppc64, wasm32);
   void wasmTruncateFloat32ToUInt64(FloatRegister input, Register64 output,
                                    bool isSaturating, Label* oolEntry,
                                    Label* oolRejoin, FloatRegister tempDouble)
-      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, wasm32);
+      DEFINED_ON(arm64, x86, x64, mips64, loong64, riscv64, ppc64, wasm32);
   void oolWasmTruncateCheckF32ToI64(FloatRegister input, Register64 output,
                                     TruncFlags flags, wasm::BytecodeOffset off,
                                     Label* rejoin)
-      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, wasm32);
+      DEFINED_ON(arm, arm64, x86_shared, mips_shared, loong64, riscv64, ppc64, wasm32);
 
   // This function takes care of loading the callee's instance and pinned regs
   // but it is the caller's responsibility to save/restore instance or pinned
   // regs.
   CodeOffset wasmCallImport(const wasm::CallSiteDesc& desc,
                             const wasm::CalleeDesc& callee);
 
 #ifdef ENABLE_WASM_TAIL_CALLS
@@ -4156,17 +4163,17 @@
   // The System ABI frequently states that the high bits of a 64-bit register
   // that holds a 32-bit return value are unpredictable, and C++ compilers will
   // indeed generate code that leaves garbage in the upper bits.
   //
   // Adjust the contents of the 64-bit register `r` to conform to our internal
   // convention, which requires predictable high bits.  In practice, this means
   // that the 32-bit value will be zero-extended or sign-extended to 64 bits as
   // appropriate for the platform.
-  void widenInt32(Register r) DEFINED_ON(arm64, x64, mips64, loong64, riscv64);
+  void widenInt32(Register r) DEFINED_ON(arm64, x64, mips64, loong64, riscv64, ppc64);
 
   // As enterFakeExitFrame(), but using register conventions appropriate for
   // wasm stubs.
   void enterFakeExitFrameForWasm(Register cxreg, Register scratch,
                                  ExitFrameType type) PER_SHARED_ARCH;
 
  public:
   // ========================================================================
@@ -4215,72 +4222,72 @@
                        const BaseIndex& mem, Register expected,
                        Register replacement, Register output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void compareExchange(Scalar::Type type, Synchronization sync,
                        const Address& mem, Register expected,
                        Register replacement, Register valueTemp,
                        Register offsetTemp, Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void compareExchange(Scalar::Type type, Synchronization sync,
                        const BaseIndex& mem, Register expected,
                        Register replacement, Register valueTemp,
                        Register offsetTemp, Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   // x86: `expected` and `output` must be edx:eax; `replacement` is ecx:ebx.
   // x64: `output` must be rax.
   // ARM: Registers must be distinct; `replacement` and `output` must be
   // (even,odd) pairs.
 
   void compareExchange64(Synchronization sync, const Address& mem,
                          Register64 expected, Register64 replacement,
                          Register64 output)
-      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64, ppc64);
 
   void compareExchange64(Synchronization sync, const BaseIndex& mem,
                          Register64 expected, Register64 replacement,
                          Register64 output)
-      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64, ppc64);
 
   // Exchange with memory.  Return the value initially in memory.
   // MIPS: `valueTemp`, `offsetTemp` and `maskTemp` must be defined for 8-bit
   // and 16-bit wide operations.
 
   void atomicExchange(Scalar::Type type, Synchronization sync,
                       const Address& mem, Register value, Register output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void atomicExchange(Scalar::Type type, Synchronization sync,
                       const BaseIndex& mem, Register value, Register output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void atomicExchange(Scalar::Type type, Synchronization sync,
                       const Address& mem, Register value, Register valueTemp,
                       Register offsetTemp, Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicExchange(Scalar::Type type, Synchronization sync,
                       const BaseIndex& mem, Register value, Register valueTemp,
                       Register offsetTemp, Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   // x86: `value` must be ecx:ebx; `output` must be edx:eax.
   // ARM: `value` and `output` must be distinct and (even,odd) pairs.
   // ARM64: `value` and `output` must be distinct.
 
   void atomicExchange64(Synchronization sync, const Address& mem,
                         Register64 value, Register64 output)
-      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64, ppc64);
 
   void atomicExchange64(Synchronization sync, const BaseIndex& mem,
                         Register64 value, Register64 output)
-      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, x64, x86, mips64, loong64, riscv64, ppc64);
 
   // Read-modify-write with memory.  Return the value in memory before the
   // operation.
   //
   // x86-shared:
   //   For 8-bit operations, `value` and `output` must have a byte subregister.
   //   For Add and Sub, `temp` must be invalid.
   //   For And, Or, and Xor, `output` must be eax and `temp` must have a byte
@@ -4304,44 +4311,44 @@
 
   void atomicFetchOp(Scalar::Type type, Synchronization sync, AtomicOp op,
                      Imm32 value, const BaseIndex& mem, Register temp,
                      Register output) DEFINED_ON(x86_shared);
 
   void atomicFetchOp(Scalar::Type type, Synchronization sync, AtomicOp op,
                      Register value, const Address& mem, Register valueTemp,
                      Register offsetTemp, Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicFetchOp(Scalar::Type type, Synchronization sync, AtomicOp op,
                      Register value, const BaseIndex& mem, Register valueTemp,
                      Register offsetTemp, Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   // x86:
   //   `temp` must be ecx:ebx; `output` must be edx:eax.
   // x64:
   //   For Add and Sub, `temp` is ignored.
   //   For And, Or, and Xor, `output` must be rax.
   // ARM:
   //   `temp` and `output` must be (even,odd) pairs and distinct from `value`.
   // ARM64:
   //   Registers `value`, `temp`, and `output` must all differ.
 
   void atomicFetchOp64(Synchronization sync, AtomicOp op, Register64 value,
                        const Address& mem, Register64 temp, Register64 output)
-      DEFINED_ON(arm, arm64, x64, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, x64, mips64, loong64, riscv64, ppc64);
 
   void atomicFetchOp64(Synchronization sync, AtomicOp op, const Address& value,
                        const Address& mem, Register64 temp, Register64 output)
       DEFINED_ON(x86);
 
   void atomicFetchOp64(Synchronization sync, AtomicOp op, Register64 value,
                        const BaseIndex& mem, Register64 temp, Register64 output)
-      DEFINED_ON(arm, arm64, x64, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, x64, mips64, loong64, riscv64, ppc64);
 
   void atomicFetchOp64(Synchronization sync, AtomicOp op, const Address& value,
                        const BaseIndex& mem, Register64 temp, Register64 output)
       DEFINED_ON(x86);
 
   // x64:
   //   `value` can be any register.
   // ARM:
@@ -4349,24 +4356,24 @@
   // ARM64:
   //   Registers `value` and `temp` must differ.
 
   void atomicEffectOp64(Synchronization sync, AtomicOp op, Register64 value,
                         const Address& mem) DEFINED_ON(x64);
 
   void atomicEffectOp64(Synchronization sync, AtomicOp op, Register64 value,
                         const Address& mem, Register64 temp)
-      DEFINED_ON(arm, arm64, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, mips64, loong64, riscv64, ppc64);
 
   void atomicEffectOp64(Synchronization sync, AtomicOp op, Register64 value,
                         const BaseIndex& mem) DEFINED_ON(x64);
 
   void atomicEffectOp64(Synchronization sync, AtomicOp op, Register64 value,
                         const BaseIndex& mem, Register64 temp)
-      DEFINED_ON(arm, arm64, mips64, loong64, riscv64);
+      DEFINED_ON(arm, arm64, mips64, loong64, riscv64, ppc64);
 
   // 64-bit atomic load. On 64-bit systems, use regular load with
   // Synchronization::Load, not this method.
   //
   // x86: `temp` must be ecx:ebx; `output` must be edx:eax.
   // ARM: `output` must be (even,odd) pair.
 
   void atomicLoad64(Synchronization sync, const Address& mem, Register64 temp,
@@ -4409,44 +4416,44 @@
                            Register replacement, Register output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void wasmCompareExchange(const wasm::MemoryAccessDesc& access,
                            const Address& mem, Register expected,
                            Register replacement, Register valueTemp,
                            Register offsetTemp, Register maskTemp,
                            Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void wasmCompareExchange(const wasm::MemoryAccessDesc& access,
                            const BaseIndex& mem, Register expected,
                            Register replacement, Register valueTemp,
                            Register offsetTemp, Register maskTemp,
                            Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void wasmAtomicExchange(const wasm::MemoryAccessDesc& access,
                           const Address& mem, Register value, Register output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void wasmAtomicExchange(const wasm::MemoryAccessDesc& access,
                           const BaseIndex& mem, Register value, Register output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void wasmAtomicExchange(const wasm::MemoryAccessDesc& access,
                           const Address& mem, Register value,
                           Register valueTemp, Register offsetTemp,
                           Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void wasmAtomicExchange(const wasm::MemoryAccessDesc& access,
                           const BaseIndex& mem, Register value,
                           Register valueTemp, Register offsetTemp,
                           Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void wasmAtomicFetchOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                          Register value, const Address& mem, Register temp,
                          Register output) DEFINED_ON(arm, arm64, x86_shared);
 
   void wasmAtomicFetchOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                          Imm32 value, const Address& mem, Register temp,
                          Register output) DEFINED_ON(x86_shared);
@@ -4458,23 +4465,23 @@
   void wasmAtomicFetchOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                          Imm32 value, const BaseIndex& mem, Register temp,
                          Register output) DEFINED_ON(x86_shared);
 
   void wasmAtomicFetchOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                          Register value, const Address& mem, Register valueTemp,
                          Register offsetTemp, Register maskTemp,
                          Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void wasmAtomicFetchOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                          Register value, const BaseIndex& mem,
                          Register valueTemp, Register offsetTemp,
                          Register maskTemp, Register output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   // Read-modify-write with memory.  Return no value.
   //
   // MIPS: `valueTemp`, `offsetTemp` and `maskTemp` must be defined for 8-bit
   // and 16-bit wide operations.
 
   void wasmAtomicEffectOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                           Register value, const Address& mem, Register temp)
@@ -4491,23 +4498,23 @@
   void wasmAtomicEffectOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                           Imm32 value, const BaseIndex& mem, Register temp)
       DEFINED_ON(x86_shared);
 
   void wasmAtomicEffectOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                           Register value, const Address& mem,
                           Register valueTemp, Register offsetTemp,
                           Register maskTemp)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void wasmAtomicEffectOp(const wasm::MemoryAccessDesc& access, AtomicOp op,
                           Register value, const BaseIndex& mem,
                           Register valueTemp, Register offsetTemp,
                           Register maskTemp)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   // 64-bit wide operations.
 
   // 64-bit atomic load.  On 64-bit systems, use regular wasm load with
   // Synchronization::Load, not this method.
   //
   // x86: `temp` must be ecx:ebx; `output` must be edx:eax.
   // ARM: `temp` should be invalid; `output` must be (even,odd) pair.
@@ -4557,22 +4564,22 @@
   // ARM: Registers must be distinct; `temp` and `output` must be (even,odd)
   // pairs.
   // MIPS: Registers must be distinct.
   // MIPS32: `temp` should be invalid.
 
   void wasmAtomicFetchOp64(const wasm::MemoryAccessDesc& access, AtomicOp op,
                            Register64 value, const Address& mem,
                            Register64 temp, Register64 output)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, x64);
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, x64);
 
   void wasmAtomicFetchOp64(const wasm::MemoryAccessDesc& access, AtomicOp op,
                            Register64 value, const BaseIndex& mem,
                            Register64 temp, Register64 output)
-      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, x64);
+      DEFINED_ON(arm, arm64, mips32, mips64, loong64, riscv64, ppc64, x64);
 
   void wasmAtomicFetchOp64(const wasm::MemoryAccessDesc& access, AtomicOp op,
                            const Address& value, const Address& mem,
                            Register64 temp, Register64 output) DEFINED_ON(x86);
 
   void wasmAtomicFetchOp64(const wasm::MemoryAccessDesc& access, AtomicOp op,
                            const Address& value, const BaseIndex& mem,
                            Register64 temp, Register64 output) DEFINED_ON(x86);
@@ -4615,44 +4622,44 @@
                          Register replacement, Register temp,
                          AnyRegister output) DEFINED_ON(arm, arm64, x86_shared);
 
   void compareExchangeJS(Scalar::Type arrayType, Synchronization sync,
                          const Address& mem, Register expected,
                          Register replacement, Register valueTemp,
                          Register offsetTemp, Register maskTemp, Register temp,
                          AnyRegister output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void compareExchangeJS(Scalar::Type arrayType, Synchronization sync,
                          const BaseIndex& mem, Register expected,
                          Register replacement, Register valueTemp,
                          Register offsetTemp, Register maskTemp, Register temp,
                          AnyRegister output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicExchangeJS(Scalar::Type arrayType, Synchronization sync,
                         const Address& mem, Register value, Register temp,
                         AnyRegister output) DEFINED_ON(arm, arm64, x86_shared);
 
   void atomicExchangeJS(Scalar::Type arrayType, Synchronization sync,
                         const BaseIndex& mem, Register value, Register temp,
                         AnyRegister output) DEFINED_ON(arm, arm64, x86_shared);
 
   void atomicExchangeJS(Scalar::Type arrayType, Synchronization sync,
                         const Address& mem, Register value, Register valueTemp,
                         Register offsetTemp, Register maskTemp, Register temp,
                         AnyRegister output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicExchangeJS(Scalar::Type arrayType, Synchronization sync,
                         const BaseIndex& mem, Register value,
                         Register valueTemp, Register offsetTemp,
                         Register maskTemp, Register temp, AnyRegister output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicFetchOpJS(Scalar::Type arrayType, Synchronization sync,
                        AtomicOp op, Register value, const Address& mem,
                        Register temp1, Register temp2, AnyRegister output)
       DEFINED_ON(arm, arm64, x86_shared);
 
   void atomicFetchOpJS(Scalar::Type arrayType, Synchronization sync,
                        AtomicOp op, Register value, const BaseIndex& mem,
@@ -4668,23 +4675,23 @@
                        AtomicOp op, Imm32 value, const BaseIndex& mem,
                        Register temp1, Register temp2, AnyRegister output)
       DEFINED_ON(x86_shared);
 
   void atomicFetchOpJS(Scalar::Type arrayType, Synchronization sync,
                        AtomicOp op, Register value, const Address& mem,
                        Register valueTemp, Register offsetTemp,
                        Register maskTemp, Register temp, AnyRegister output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicFetchOpJS(Scalar::Type arrayType, Synchronization sync,
                        AtomicOp op, Register value, const BaseIndex& mem,
                        Register valueTemp, Register offsetTemp,
                        Register maskTemp, Register temp, AnyRegister output)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicEffectOpJS(Scalar::Type arrayType, Synchronization sync,
                         AtomicOp op, Register value, const Address& mem,
                         Register temp) DEFINED_ON(arm, arm64, x86_shared);
 
   void atomicEffectOpJS(Scalar::Type arrayType, Synchronization sync,
                         AtomicOp op, Register value, const BaseIndex& mem,
                         Register temp) DEFINED_ON(arm, arm64, x86_shared);
@@ -4696,23 +4703,23 @@
   void atomicEffectOpJS(Scalar::Type arrayType, Synchronization sync,
                         AtomicOp op, Imm32 value, const BaseIndex& mem,
                         Register temp) DEFINED_ON(x86_shared);
 
   void atomicEffectOpJS(Scalar::Type arrayType, Synchronization sync,
                         AtomicOp op, Register value, const Address& mem,
                         Register valueTemp, Register offsetTemp,
                         Register maskTemp)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicEffectOpJS(Scalar::Type arrayType, Synchronization sync,
                         AtomicOp op, Register value, const BaseIndex& mem,
                         Register valueTemp, Register offsetTemp,
                         Register maskTemp)
-      DEFINED_ON(mips_shared, loong64, riscv64);
+      DEFINED_ON(mips_shared, loong64, riscv64, ppc64);
 
   void atomicIsLockFreeJS(Register value, Register output);
 
   // ========================================================================
   // Spectre Mitigations.
   //
   // Spectre attacks are side-channel attacks based on cache pollution or
   // slow-execution of some instructions. We have multiple spectre mitigations
@@ -5711,17 +5718,17 @@
   // On ARM64, the StackPointer is implemented as two synchronized registers.
   // Code shared across platforms must use these functions to be valid.
   template <typename T>
   inline void addToStackPtr(T t);
   template <typename T>
   inline void addStackPtrTo(T t);
 
   void subFromStackPtr(Imm32 imm32)
-      DEFINED_ON(mips32, mips64, loong64, riscv64, wasm32, arm, x86, x64);
+      DEFINED_ON(mips32, mips64, loong64, riscv64, ppc64, wasm32, arm, x86, x64);
   void subFromStackPtr(Register reg);
 
   template <typename T>
   void subStackPtrFrom(T t) {
     subPtr(getStackPointer(), t);
   }
 
   template <typename T>
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/MoveEmitter.h
--- a/js/src/jit/MoveEmitter.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/MoveEmitter.h	Thu Aug 08 21:25:14 2024 -0700
@@ -16,16 +16,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/MoveEmitter-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/MoveEmitter-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/MoveEmitter-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/MoveEmitter-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/MoveEmitter-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/MoveEmitter-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/MoveEmitter-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/RegisterAllocator.h
--- a/js/src/jit/RegisterAllocator.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/RegisterAllocator.h	Thu Aug 08 21:25:14 2024 -0700
@@ -289,17 +289,17 @@
   void dumpInstructions(const char* who);
 
  public:
   template <typename TakeableSet>
   static void takeWasmRegisters(TakeableSet& regs) {
 #if defined(JS_CODEGEN_X64) || defined(JS_CODEGEN_ARM) ||        \
     defined(JS_CODEGEN_ARM64) || defined(JS_CODEGEN_MIPS32) ||   \
     defined(JS_CODEGEN_MIPS64) || defined(JS_CODEGEN_LOONG64) || \
-    defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_RISCV64) || defined(JS_CODEGEN_PPC64)
     regs.take(HeapReg);
 #endif
     MOZ_ASSERT(!regs.has(FramePointer));
   }
 };
 
 static inline AnyRegister GetFixedRegister(const LDefinition* def,
                                            const LUse* use) {
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/Registers.h
--- a/js/src/jit/Registers.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/Registers.h	Thu Aug 08 21:25:14 2024 -0700
@@ -19,16 +19,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/Architecture-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/Architecture-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/Architecture-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/Architecture-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/Architecture-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/Architecture-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/Architecture-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/SharedICHelpers-inl.h
--- a/js/src/jit/SharedICHelpers-inl.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/SharedICHelpers-inl.h	Thu Aug 08 21:25:14 2024 -0700
@@ -16,16 +16,18 @@
 #elif defined(JS_CODEGEN_ARM64)
 #  include "jit/arm64/SharedICHelpers-arm64-inl.h"
 #elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips-shared/SharedICHelpers-mips-shared-inl.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/SharedICHelpers-loong64-inl.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/SharedICHelpers-riscv64-inl.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/SharedICHelpers-ppc64-inl.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/SharedICHelpers-wasm32-inl.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/SharedICHelpers-none-inl.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/SharedICHelpers.h
--- a/js/src/jit/SharedICHelpers.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/SharedICHelpers.h	Thu Aug 08 21:25:14 2024 -0700
@@ -16,16 +16,18 @@
 #elif defined(JS_CODEGEN_ARM64)
 #  include "jit/arm64/SharedICHelpers-arm64.h"
 #elif defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips-shared/SharedICHelpers-mips-shared.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/SharedICHelpers-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/SharedICHelpers-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/SharedICHelpers-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/SharedICHelpers-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/SharedICHelpers-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/SharedICRegisters.h
--- a/js/src/jit/SharedICRegisters.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/SharedICRegisters.h	Thu Aug 08 21:25:14 2024 -0700
@@ -18,16 +18,18 @@
 #elif defined(JS_CODEGEN_MIPS32)
 #  include "jit/mips32/SharedICRegisters-mips32.h"
 #elif defined(JS_CODEGEN_MIPS64)
 #  include "jit/mips64/SharedICRegisters-mips64.h"
 #elif defined(JS_CODEGEN_LOONG64)
 #  include "jit/loong64/SharedICRegisters-loong64.h"
 #elif defined(JS_CODEGEN_RISCV64)
 #  include "jit/riscv64/SharedICRegisters-riscv64.h"
+#elif defined(JS_CODEGEN_PPC64)
+#  include "jit/ppc64/SharedICRegisters-ppc64.h"
 #elif defined(JS_CODEGEN_WASM32)
 #  include "jit/wasm32/SharedICRegisters-wasm32.h"
 #elif defined(JS_CODEGEN_NONE)
 #  include "jit/none/SharedICRegisters-none.h"
 #else
 #  error "Unknown architecture!"
 #endif
 
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/moz.build
--- a/js/src/jit/moz.build	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/moz.build	Thu Aug 08 21:25:14 2024 -0700
@@ -250,16 +250,26 @@
         "riscv64/extension/extension-riscv-zifencei.cc",
         "riscv64/Lowering-riscv64.cpp",
         "riscv64/MacroAssembler-riscv64.cpp",
         "riscv64/MoveEmitter-riscv64.cpp",
         "riscv64/Trampoline-riscv64.cpp",
     ]
     if CONFIG["JS_SIMULATOR_RISCV64"]:
         UNIFIED_SOURCES += ["riscv64/Simulator-riscv64.cpp"]
+elif CONFIG["JS_CODEGEN_PPC64"]:
+    UNIFIED_SOURCES += [
+        "ppc64/Architecture-ppc64.cpp",
+        "ppc64/Assembler-ppc64.cpp",
+        "ppc64/CodeGenerator-ppc64.cpp",
+        "ppc64/Lowering-ppc64.cpp",
+        "ppc64/MacroAssembler-ppc64.cpp",
+        "ppc64/MoveEmitter-ppc64.cpp",
+        "ppc64/Trampoline-ppc64.cpp",
+    ]
 elif CONFIG["JS_CODEGEN_WASM32"]:
     UNIFIED_SOURCES += [
         "wasm32/CodeGenerator-wasm32.cpp",
         "wasm32/MacroAssembler-wasm32.cpp",
         "wasm32/Trampoline-wasm32.cpp",
     ]
 
 # Generate jit/ABIFunctionTypeGenerated.h from jit/ABIFunctionType.yaml
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/shared/Assembler-shared.h
--- a/js/src/jit/shared/Assembler-shared.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/shared/Assembler-shared.h	Thu Aug 08 21:25:14 2024 -0700
@@ -24,24 +24,24 @@
 #include "js/ScalarType.h"  // js::Scalar::Type
 #include "vm/HelperThreads.h"
 #include "wasm/WasmCodegenTypes.h"
 #include "wasm/WasmConstants.h"
 
 #if defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_ARM64) ||      \
     defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) ||  \
     defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_WASM32) || \
-    defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_RISCV64) || defined(JS_CODEGEN_PPC64)
 // Push return addresses callee-side.
 #  define JS_USE_LINK_REGISTER
 #endif
 
 #if defined(JS_CODEGEN_MIPS32) || defined(JS_CODEGEN_MIPS64) || \
     defined(JS_CODEGEN_ARM64) || defined(JS_CODEGEN_LOONG64) || \
-    defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_RISCV64) || defined(JS_CODEGEN_PPC64)
 // JS_CODELABEL_LINKMODE gives labels additional metadata
 // describing how Bind() should patch them.
 #  define JS_CODELABEL_LINKMODE
 #endif
 
 using js::wasm::FaultingCodeOffset;
 
 namespace js {
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/shared/AtomicOperations-shared-jit.cpp
--- a/js/src/jit/shared/AtomicOperations-shared-jit.cpp	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/shared/AtomicOperations-shared-jit.cpp	Thu Aug 08 21:25:14 2024 -0700
@@ -54,16 +54,19 @@
 #  endif
 #  if defined(__x86_64__) || defined(__i386__)
   return true;
 #  elif defined(__arm__)
   return !HasAlignmentFault();
 #  elif defined(__aarch64__)
   // This is not necessarily true but it's the best guess right now.
   return true;
+#  elif defined(__powerpc__) || defined(__powerpc64__) || defined(__ppc__)
+  // Unaligned accesses are supported in hardware (just suboptimal).
+  return true;
 #  else
 #    error "Unsupported platform"
 #  endif
 }
 
 #  ifndef JS_64BIT
 void AtomicCompilerFence() {
   std::atomic_signal_fence(std::memory_order_acq_rel);
diff -r 553c3f1c48b4 -r 9a197bcbc770 js/src/jit/shared/Lowering-shared-inl.h
--- a/js/src/jit/shared/Lowering-shared-inl.h	Thu Aug 08 21:24:28 2024 -0700
+++ b/js/src/jit/shared/Lowering-shared-inl.h	Thu Aug 08 21:25:14 2024 -0700
@@ -518,17 +518,17 @@
       mir->type() != MIRType::Float32) {
     return LAllocation(mir->toConstant());
   }
   return useRegister(mir);
 }
 
 #if defined(JS_CODEGEN_ARM) || defined(JS_CODEGEN_ARM64) ||      \
     defined(JS_CODEGEN_LOONG64) || defined(JS_CODEGEN_MIPS64) || \
-    defined(JS_CODEGEN_RISCV64)
+    defined(JS_CODEGEN_RISCV64) || defined(JS_CODEGEN_PPC64)
 LAllocation LIRGeneratorShared::useAnyOrConstant(MDefinition* mir) {
   return useRegisterOrConstant(mir);
 }
 LAllocation LIRGeneratorShared::useStorable(MDefinition* mir) {
   return useRegister(mir);
 }
 LAllocation LIRGeneratorShared::useStorableAtStart(MDefinition* mir) {
   return useRegisterAtStart(mir);
